# –ó–∞–≥—Ä—É–∑–∫–∞ –æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
cargo run -- ingest data/doc1.txt

# –ó–∞–≥—Ä—É–∑–∫–∞ –≤—Å–µ–π –ø–∞–ø–∫–∏
cargo run -- ingest data/

# –ó–∞–≥—Ä—É–∑–∫–∞ —Å —É–∫–∞–∑–∞–Ω–∏–µ–º –∫–æ–ª–ª–µ–∫—Ü–∏–∏
cargo run -- ingest data/ --collection my_docs

# –ü–æ–∏—Å–∫ (–∫–æ–≥–¥–∞ —Ä–µ–∞–ª–∏–∑—É–µ–º)
cargo run -- query "–Ω–∞—Å—Ç—Ä–æ–π–∫–∞ VPN"

https://huggingface.co/intfloat/multilingual-e5-base/blob/a114a4100c6714cf21651971eefe9191a4415dbb/onnx/model.onnx?utm_source=chatgpt.com

wget https://huggingface.co/intfloat/multilingual-e5-base/resolve/main/tokenizer.json
wget https://huggingface.co/intfloat/multilingual-e5-base/resolve/main/tokenizer_config.json
wget https://huggingface.co/intfloat/multilingual-e5-base/resolve/main/special_tokens_map.json
wget https://huggingface.co/intfloat/multilingual-e5-base/resolve/main/sentencepiece.bpe.model
wget https://huggingface.co/intfloat/multilingual-e5-base/resolve/main/onnx/model.onnx

–°–æ–¥–µ—Ä–∂–∏—Ç hidden_size, max_position_embeddings, vocab_size, architectures, model_type, layer_norm_eps –∏ —Ç.–¥. ‚Äî –Ω—É–∂–µ–Ω, –µ—Å–ª–∏ —Ç—ã —Ö–æ—á–µ—à—å –≤ Rust —Å–æ–±—Ä–∞—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä—É –º–æ–¥–µ–ª–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –¥–ª—è tch –∏–ª–∏ onnx shape checking).
https://huggingface.co/intfloat/multilingual-e5-base/resolve/main/config.json

–ù–µ–æ–±—è–∑–∞—Ç–µ–ª–µ–Ω, –Ω–æ –ø–æ–ª–µ–∑–µ–Ω: —Ç–∞–º —Ö—Ä–∞–Ω–∏—Ç—Å—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ç–æ–º, –∫–∞–∫ pooling –¥–µ–ª–∞–µ—Ç—Å—è (mean/cls), –∫–∞–∫–∏–µ normalization —à–∞–≥–∏ –ø—Ä–∏–º–µ–Ω—è—é—Ç—Å—è. –ï—Å–ª–∏ —Ç—ã —Ö–æ—á–µ—à—å 100% —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —Å Python, –Ω—É–∂–µ–Ω.
https://huggingface.co/intfloat/multilingual-e5-base/resolve/main/modules.json


onnxruntime
https://github.com/microsoft/onnxruntime/releases/latest

F:\libs\onnxruntime-win-x64-gpu-1.23.2\lib

$env:ORT_USE="directml" # –µ—Å–ª–∏ –±–µ–∑ CUDA

$env:ORT_USE="cuda"
$env:ORT_DYLIB_PATH="F:\libs\onnxruntime-win-x64-gpu-1.23.2\lib"
$env:PATH = "$env:ORT_DYLIB_PATH;$env:PATH"

–æ—Ç—Å—Ç—Ä–µ–ª–∏—Ç—å rust-analyzer
taskkill /F /IM rust-analyzer.exe 2>$null


cargo run --release -- ^
  --model-dir "models/multilingual-e5-base" ^
  --tokenizer-path "models/multilingual-e5-base/tokenizer.json" ^
  --text "–ü—Ä–∏–≤–µ—Ç, –º–∏—Ä! –ü—Ä–æ–≤–µ—Ä–∫–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤."

------

# –µ—Å–ª–∏ –Ω—É–∂–Ω–∞ GPU-–≤–µ—Ä—Å–∏—è
$env:ORT_USE="cuda"            # –∏–ª–∏ "directml" –Ω–∞ Windows –±–µ–∑ CUDA
$env:ORT_DYLIB_PATH="C:\tools\onnxruntime-win-x64-gpu-1.17.0\lib"
$env:PATH="$env:ORT_DYLIB_PATH;$env:PATH"

cargo clean
cargo run --release -- `
  --model-dir models `
  --tokenizer-path models/tokenizer.json `
  --text "–ü—Ä–∏–≤–µ—Ç, –º–∏—Ä! –ü—Ä–æ–≤–µ—Ä–∫–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤."

—Å–æ–∑–¥–∞—Ç—å –∫–æ–ª–ª–µ–∫—Ü–∏—é –≤ Qdrant

curl -X PUT "http://127.0.0.1:6333/collections/chunks" `
  -H "Content-Type: application/json" `
  -d '{
    "vectors": { "size": 768, "distance": "Cosine" },
    "optimizers_config": { "default_segment_number": 2 }
  }'

Invoke-RestMethod -Uri "http://127.0.0.1:6333/collections/chunks" `
  -Method PUT `
  -ContentType "application/json" `
  -Body '{
    "vectors": {
      "size": 768,
      "distance": "Cosine"
    }
  }'

Invoke-RestMethod -Uri "http://127.0.0.1:6333/collections" -Method GET



cargo run --release --bin ingest_qdrant -- `
  --text "–ö–æ–º–ø–∞–∫—Ü–∏—è –≤ LSM-tree –±—ã–≤–∞–µ—Ç leveled –∏ tiered." `
  --source-id "samples/lsm-notes.md" `
  --collection "chunks" `
  --model-dir "models" `
  --tokenizer-path "models/tokenizer.json" `
  --qdrant-host "127.0.0.1" `
  --qdrant-grpc-port 6334


cargo run --release --bin search_qdrant -- `
  --query "–∫–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–æ–º–ø–∞–∫—Ü–∏—è –≤ LSM-tree?" `
  --collection "chunks" `
  --model-dir "models" `
  --tokenizer-path "models/tokenizer.json" `
  --qdrant-host "127.0.0.1" `
  --qdrant-grpc-port 6334 `
  --top-k 5 `
  --score-threshold 0.2 `
  --with-payload true

cargo run --release --bin search_qdrant -- `
  --query "–∫–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–æ–º–ø–∞–∫—Ü–∏—è –≤ LSM-tree?" `
  --collection chunks `
  --model-dir models `
  --tokenizer-path models/tokenizer.json `
  --qdrant-host 127.0.0.1 `
  --qdrant-grpc-port 6334 `
  --top-k 5 `
  --with-payload

batch-ingest

cargo run --release --bin ingest_qdrant -- `
  --input-dir "./notes" `
  --collection "chunks" `
  --model-dir "models" `
  --tokenizer-path "models/tokenizer.json" `
  --qdrant-host "127.0.0.1" `
  --qdrant-grpc-port 6334

--------


–¢—é–Ω–∏–Ω–≥ –∫–æ–ª–ª–µ–∫—Ü–∏–∏ (HNSW + –æ–ø—Ç–∏–º–∞–π–∑–µ—Ä)

Invoke-RestMethod -Uri "http://127.0.0.1:6333/collections/chunks/params" `
  -Method PATCH -ContentType "application/json" `
  -Body '{
    "hnsw_config": { "m": 16, "ef_construct": 200 },
    "optimizer_config": { "memmap_threshold": 200000, "default_segment_number": 2 },
    "quantization_config": null
  }'

----


$env:QDRANT_HOST="127.0.0.1"
$env:QDRANT_GRPC_PORT="6334"

cargo run --release --bin ingest_qdrant_grpc_chunked -- `
  --input-dir .\samples `
  --source-id "file://samples" `
  --collection chunks `
  --model-dir .\models\multilingual-e5-base `
  --tokenizer-path .\models\multilingual-e5-base\tokenizer.json `
  --wait

QDRANT_HOST=127.0.0.1 QDRANT_GRPC_PORT=6334 \
cargo run --release --bin ingest_qdrant_grpc_chunked \
  --input-dir ./samples \
  --source-id "file://samples" \
  --collection chunks \
  --model-dir ./models/multilingual-e5-base \
  --tokenizer-path ./models/multilingual-e5-base/tokenizer.json \
  --wait

ingest_qdrant_chunked_v3.rs

cargo run --release --bin ingest_qdrant_chunked_v3 -- `
  --input-dir ./notes `
  --source-id "file://samples" `
  --collection chunks `
  --model-dir ./models `
  --tokenizer-path ./models/tokenizer.json

---------

docker run -p 6333:6333 -p 6334:6334 qdrant/qdrant

# –û—Ç–∫—Ä–æ–π—Ç–µ –≤ –±—Ä–∞—É–∑–µ—Ä–µ
http://localhost:6333/dashboard


# –ò–ª–∏ —Å –ø–æ–ª–Ω—ã–º–∏ –ø—É—Ç—è–º–∏ –∫ –º–æ–¥–µ–ª–∏
cargo run --bin ingest -- `
  --input-dir ./notes `
  --collection chunks `
  --model-dir ./models/multilingual-e5-base `
  --tokenizer-path ./models/multilingual-e5-base/tokenizer.json `
  --source-id "file://notes"


cargo run --bin verify -- --collection chunks
```

–í—ã–≤–æ–¥ –±—É–¥–µ—Ç –ø—Ä–∏–º–µ—Ä–Ω–æ —Ç–∞–∫–∏–º:
```
üîç Analyzing collection: chunks

üìä Total points: 180
üìö Total documents: 2
üìÑ Total chunks: 180

üîé Checking for duplicate chunks...
‚úÖ No duplicate chunks found!

üìä Top 5 documents by chunk count:
  1. doc::230a81772d646590a - 130 chunks
  2. doc::9bc50254b1f95d058 - 50 chunks

üìÅ Documents by source:
  file://samples/ai_article.txt - 130 chunks
  file://samples/rust_guide.txt - 50 chunks

‚ú® Verification complete!


# –ó–∞–ø—É—Å—Ç–∏—Ç–µ ingest –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ä–∞–∑ –Ω–∞ —Ç–µ –∂–µ —Ñ–∞–π–ª—ã
cargo run --bin ingest -- --input-dir ./notes
cargo run --bin ingest -- --input-dir ./notes
cargo run --bin ingest -- --input-dir ./notes

# –ü—Ä–æ–≤–µ—Ä—å—Ç–µ - –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–ª–∂–Ω–æ –æ—Å—Ç–∞—Ç—å—Å—è —Ç–µ–º –∂–µ!
cargo run --bin verify


------

# –î–ª—è –∫–æ—Ä–æ—Ç–∫–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (—Å—Ç–∞—Ç—å–∏, –∑–∞–º–µ—Ç–∫–∏)
cargo run --bin ingest -- \
  --input-dir ./docs \
  --max-tokens 250 \
  --overlap-tokens 40

# –î–ª—è –¥–ª–∏–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (–∫–Ω–∏–≥–∏, –æ—Ç—á—ë—Ç—ã)
cargo run --bin ingest -- \
  --input-dir ./books \
  --max-tokens 500 \
  --overlap-tokens 100

--------

–ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ —á–∞–Ω–∫–∏–Ω–≥–∞

# –ü–æ—Å–º–æ—Ç—Ä–∏—Ç–µ –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã–µ —á–∞–Ω–∫–∏
cargo run --bin verify -- --collection chunks

# –ü–æ–∏—â–∏—Ç–µ —á—Ç–æ-—Ç–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–µ
cargo run --bin search -- "rust programming" --limit 10

# –ü–æ–ª—É—á–∏—Ç–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è RAG
cargo run --bin search -- "machine learning" --context --limit 5


cargo run --release --bin rag -- "test question" --stream


-----

—á–∏—Å—Ç–∏–º –∫–æ–ª–ª–µ–∫—Ü–∏—é clean collection

curl -X DELETE "http://127.0.0.1:6333/collections/chunks"
Invoke-RestMethod -Uri "http://127.0.0.1:6333/collections/chunks" -Method DELETE


—Ç–µ–∫—Å—Ç

cargo run --bin ingest -- `
  --text "–ö–æ–º–ø–∞–∫—Ü–∏—è –≤ LSM-tree –±—ã–≤–∞–µ—Ç leveled –∏ tiered." `
  --collection chunks `
  --model-dir ./models/multilingual-e5-base `
  --tokenizer-path ./models/multilingual-e5-base/tokenizer.json `
  --source-id "file://notes"

–ø–∞–ø–∫—É

cargo run --bin ingest -- `
  --input-dir ./docs `
  --collection chunks `
  --model-dir ./models/multilingual-e5-base `
  --tokenizer-path ./models/multilingual-e5-base/tokenizer.json `
  --source-id "file://notes"

cargo run --bin search -- "lsm" --limit 10


# 510 = 512 - 2 ([CLS] –∏ [SEP]) ‚Äî –±–µ–∑–æ–ø–∞—Å–Ω–æ
$env:HYBRID_CHUNK_MAX_TOKENS = "510"
$env:HYBRID_CHUNK_OVERLAP   = "40"   # –∫–∞–∫ —É–¥–æ–±–Ω–æ
cargo run --release

$res = Invoke-RestMethod `
   -Uri "http://127.0.0.1:8090/api/ingest/text_raw?title=raw_utf16_ok" `
   -Method POST `
   -ContentType "text/plain" `
   -Body $bytes